import requests
import json
import logging
import os
from fastapi import FastAPI, Request, HTTPException
from pydantic import BaseModel
from typing import Optional, List, Dict, Any

# Importa a nova ferramenta de consulta
from gaya_db_query_tool import TOOL_SCHEMA, TOOL_FUNCTIONS

# --- Configura√ß√£o de Logging ---
# Configura√ß√£o base para mostrar logs no terminal
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s: %(message)s')
logger = logging.getLogger('GAYA_API')

# --- Configura√ß√£o do LLM (Ollama) ---
OLLAMA_HOST = os.getenv('OLLAMA_HOST', 'http://127.0.0.1:11434')
OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'llama3.2:1b') # Seu modelo mais leve

# --- Configura√ß√£o do FastAPI ---
app = FastAPI(title="GAYA - API do LLM com Function Calling")

# --- Modelo de Dados ---
class Message(BaseModel):
    user_id: int
    username: Optional[str] = "Usu√°rio Desconhecido"
    text: str

# --- Fun√ß√µes de Comunica√ß√£o com Ollama ---

def _call_ollama_api(payload: Dict[str, Any]) -> Dict[str, Any]:
    """Fun√ß√£o auxiliar para chamar a API do Ollama."""
    url = f"{OLLAMA_HOST}/api/generate"
    try:
        response = requests.post(url, json=payload, stream=False)
        response.raise_for_status() # Lan√ßa exce√ß√£o para status ruins (4xx, 5xx)
        
        # O Ollama, por padr√£o, retorna JSONs em cada linha, mas 
        # para a API /generate com stream=False ele retorna o objeto completo.
        # Precisamos parsear a resposta completa
        full_response = response.json()
        
        # O campo 'response' cont√©m o texto final ou o JSON da chamada de fun√ß√£o
        return full_response
        
    except requests.exceptions.RequestException as e:
        logger.error(f"Erro de comunica√ß√£o com Ollama: {e}")
        # Retorna um formato de erro que o loop principal possa gerenciar
        return {"error": f"Erro de comunica√ß√£o com LLM: {e}"}

def _get_llm_response(prompt: str, tools_schema: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Envia o prompt e o schema das ferramentas ao Ollama.
    
    CORRE√á√ÉO: Este m√©todo agora detecta palavras-chave e, se houver, 
    usa um hist√≥rico de conversa√ß√£o pr√©-preenchido para *for√ßar* o modelo a 
    reconhecer a inten√ß√£o de usar a ferramenta na primeira chamada.
    
    Isso √© necess√°rio porque modelos menores (1b) podem ignorar o `systemInstruction`.
    """
    
    # 1. Palavras-chave para for√ßar o uso da ferramenta
    tool_keywords = ["quantos", "cargas", "fretes", "total", "status"]
    
    # 2. Verifica se o prompt cont√©m alguma palavra-chave (case-insensitive)
    needs_tool = any(kw in prompt.lower() for kw in tool_keywords)
    
    # --- Cria√ß√£o do Hist√≥rico (messages) ---
    
    # Sistema: Instru√ß√£o de personalidade e objetivo
    system_message = {
        "role": "system",
        "content": (
            "Voc√™ √© a GAYA, uma IA de log√≠stica com personalidade debochada e firme, "
            "mas extremamente eficiente. Sua miss√£o √© auxiliar o usu√°rio com informa√ß√µes "
            "de fretes e cargas. "
            "Sua tarefa √© analisar o prompt do usu√°rio e decidir se a ferramenta deve ser usada."
        )
    }
    
    # Usu√°rio: O prompt original
    user_message = {
        "role": "user",
        "content": prompt
    }

    messages = [system_message]
    
    if needs_tool:
        logger.warning("FOR√áANDO: LLM ser√° for√ßado a solicitar a tool 'consultar_status_geral_db'...")
        
        # Para for√ßar o LLM a chamar a fun√ß√£o, criamos um hist√≥rico artificial
        # onde o "assistant" *j√°* solicitou a chamada da fun√ß√£o.
        # Na verdade, a API do Ollama ir√° interpretar isso como um pedido de 
        # Fun√ß√£o Chamada para dar prosseguimento.
        
        # Adiciona a mensagem do usu√°rio
        messages.append(user_message)
        
        # Simula que o assistente j√° decidiu e chamou a fun√ß√£o
        messages.append({
            "role": "assistant",
            "content": None,
            "tool_calls": [
                {
                    "function": {
                        "name": "consultar_status_geral_db",
                        "arguments": {}
                    }
                }
            ]
        })
        
        # O payload para o LLM na verdade ser√° apenas o hist√≥rico, sem o 'tools'
        # Isso faz o LLM entrar no modo "executar a tool e dar a resposta final"
        tools_list_for_payload = None 
        
    else:
        # Se n√£o precisar de tool, faz a chamada normal para obter a resposta direta
        messages.append(user_message)
        tools_list_for_payload = tools_schema

    
    payload = {
        "model": OLLAMA_MODEL,
        "messages": messages,
        "options": {
            "temperature": 0.7 
        },
        "tools": tools_list_for_payload, # Inclui o schema se n√£o estiver for√ßando
        "stream": False 
    }
    
    return _call_ollama_api(payload)

def _process_function_call(response: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """Processa a resposta do Ollama para executar uma fun√ß√£o, se solicitada."""
    
    # Verifica se o Ollama solicitou uma chamada de fun√ß√£o
    # OBS: Quando *for√ßamos* a chamada no _get_llm_response, a fun√ß√£o
    # que √© executada √© a do *nosso* c√≥digo, e o resultado √© passado
    # para a segunda chamada do LLM. O Ollama n√£o retorna a tool_call neste caso for√ßado,
    # ele espera o resultado da tool.
    
    # Vamos adaptar a l√≥gica aqui para o cen√°rio for√ßado,
    # que √© o √∫ltimo item da lista de mensagens.
    
    # Se a resposta do LLM na primeira chamada vier vazia ou sem action, 
    # e n√≥s detectamos a necessidade de tool, precisamos simular o action aqui.
    
    if 'actions' in response and response['actions']:
        tool_call = response['actions'][0]
        tool_name = tool_call.get('function', {}).get('name')
        tool_args = tool_call.get('function', {}).get('arguments', {})
        
        logger.info(f"ü§ñ LLM solicitou chamada de fun√ß√£o: {tool_name} com args: {tool_args}")

        if tool_name in TOOL_FUNCTIONS:
            # Encontra a fun√ß√£o Python correspondente
            func = TOOL_FUNCTIONS[tool_name]
            
            try:
                # Executa a fun√ß√£o Python (Tool)
                result = func(**tool_args)
                logger.info("‚úÖ Ferramenta executada com sucesso.")
                return {
                    "tool_name": tool_name,
                    "result": result
                }
            except Exception as e:
                logger.error(f"‚ùå Erro ao executar fun√ß√£o {tool_name}: {e}")
                return {
                    "tool_name": tool_name,
                    "result": {"error": f"Erro interno ao executar a ferramenta: {str(e)}"}
                }
        else:
            logger.error(f"‚ùå Fun√ß√£o solicitada '{tool_name}' n√£o mapeada.")
            # Continuar com resposta direta se o LLM alucinar uma tool
            return None 

    # Se n√£o houve 'actions' na resposta do Ollama, mas o LLM deu uma resposta direta
    # (caso n√£o tenha detectado a necessidade de tool), retornamos None para
    # que o fluxo caia na "Resposta Direta"
    return None

def _get_final_response_after_tool(
    prompt: str, 
    tool_name: str, 
    tool_output: Dict[str, Any],
    tools_schema: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """Envia o resultado da fun√ß√£o de volta ao LLM para gerar a resposta final."""

    # Prepara o hist√≥rico da conversa com o resultado da tool
    messages = [
        {
            "role": "system",
            "content": (
                "Voc√™ √© a GAYA, uma IA de log√≠stica com personalidade debochada e firme, "
                "mas extremamente eficiente. Sua miss√£o √© auxiliar o usu√°rio com informa√ß√µes "
                "de fretes e cargas. "
                "Use o resultado da ferramenta para gerar uma resposta final, relevante, "
                "debochada e √∫til para o usu√°rio. N√ÉO inclua o JSON de sa√≠da da ferramenta "
                "na resposta final."
            )
        },
        {
            "role": "user",
            "content": prompt
        },
        {
            "role": "assistant",
            "content": None, 
            "tool_calls": [
                {
                    "function": {
                        "name": tool_name,
                        "arguments": {} 
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": json.dumps(tool_output) # O resultado da tool
        }
    ]

    # Novo payload com o hist√≥rico completo
    payload = {
        "model": OLLAMA_MODEL,
        "messages": messages,
        "options": {
            "temperature": 0.7 
        },
        "stream": False # S√≠ncrono para resposta final
    }
    
    return _call_ollama_api(payload)


# --- Rota Principal ---

@app.post("/mensagem")
async def handle_message(message: Message, request: Request):
    """
    Rota principal que recebe a mensagem do Telegram e gerencia o ciclo
    de racioc√≠nio do LLM com o Function Calling.
    """
    
    # 1. Loga a mensagem recebida
    username_log = message.username or f"Usu√°rio ID: {message.user_id}"
    logger.info(f"GAYA_API: Recebi mensagem de {username_log} (Telegram): {message.text}")
    
    user_prompt = message.text
    tools_list = [TOOL_SCHEMA]
    
    # --- VERIFICA√á√ÉO DE CHAVE PARA O C√ìDIGO FOR√áADO ---
    tool_keywords = ["quantos", "cargas", "fretes", "total", "status"]
    needs_tool_forced = any(kw in user_prompt.lower() for kw in tool_keywords)
    # --------------------------------------------------

    # 2. Primeira chamada ao LLM: Decis√£o de Tool
    llm_response = _get_llm_response(user_prompt, tools_list)

    if 'error' in llm_response:
        return {"response": f"‚ùå ERRO LLM: {llm_response['error']}"}

    
    # 3. Processamento do Function Call (Se o LLM solicitou ou se foi for√ßado)
    
    tool_data = _process_function_call(llm_response)

    # L√≥gica de fallback para a execu√ß√£o for√ßada
    if needs_tool_forced and not tool_data:
        # Se detectamos a necessidade de tool, mas o LLM n√£o explicitou a chamada (vazio ou resposta direta)
        # O _get_llm_response j√° preparou o hist√≥rico para for√ßar a chamada,
        # mas precisamos simular o resultado do _process_function_call
        
        tool_name = "consultar_status_geral_db"
        logger.info(f"‚öôÔ∏è EXECUTANDO FOR√áADO: Chamando a fun√ß√£o {tool_name} diretamente...")

        if tool_name in TOOL_FUNCTIONS:
            func = TOOL_FUNCTIONS[tool_name]
            try:
                tool_output_forced = func()
                logger.info("‚úÖ Ferramenta executada (For√ßada) com sucesso.")
                tool_data = {
                    "tool_name": tool_name,
                    "result": tool_output_forced
                }
            except Exception as e:
                logger.error(f"‚ùå Erro ao executar fun√ß√£o (For√ßada) {tool_name}: {e}")
                tool_data = {
                    "tool_name": tool_name,
                    "result": {"error": f"Erro interno ao executar a ferramenta (For√ßada): {str(e)}"}
                }


    if tool_data:
        # Se uma fun√ß√£o foi chamada (seja pelo LLM, seja por detec√ß√£o for√ßada)
        tool_name = tool_data['tool_name']
        tool_output = tool_data['result']

        # 4. Segunda Chamada ao LLM: Gera√ß√£o da Resposta Final
        final_response = _get_final_response_after_tool(
            user_prompt, 
            tool_name, 
            tool_output,
            tools_list
        )
        
        if 'error' in final_response:
            return {"response": f"‚ùå ERRO LLM na resposta final: {final_response['error']}"}
        
        final_text = final_response.get('response', 'GAYA est√° sem palavras (Resposta final vazia do LLM).')
        
        logger.info(f"‚úÖ Gera√ß√£o final ap√≥s Tool. Resposta: {final_text[:50]}...")
        return {"response": final_text}

    else:
        # 5. Resposta Direta (Se o LLM N√ÉO solicitou Tool E N√ÉO foi for√ßado)
        final_text = llm_response.get('response', 'GAYA est√° sem palavras (Resposta direta vazia do LLM).')
        logger.info(f"‚û°Ô∏è Resposta direta do LLM. Resposta: {final_text[:50]}...")
        return {"response": final_text}
